{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e891843-c0b4-48dd-9347-c4613e46defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pickle import dump\n",
    "import numpy as np\n",
    "import h5py\n",
    "import flows as flows\n",
    "from layers import MaskedConv2d, MaskedLinear, CNN_Flow_Layer, Dilation_Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e2cb48-805a-4192-b0ac-bfc39a08e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "     class ConvNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(ConvNet, self).__init__()\n",
    "\n",
    "            self.latent_dim = 20\n",
    "            self.batch_size = 100                \n",
    "            self.q_z_output_dim = 128\n",
    "            self.z_size = 20\n",
    "            self.test_mode = 0\n",
    "    \n",
    "            self.q_z_nn = nn.Sequential(\n",
    "                  nn.Conv2d(1, 32, kernel_size=(3,4), stride=(1), padding=(0)),\n",
    "                  nn.BatchNorm2d(32),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(32, 16, kernel_size=(5,1), stride=(1), padding=(0)),\n",
    "                  nn.BatchNorm2d(16),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(16, 8, kernel_size=(7,1), stride=(1), padding=(0)),\n",
    "                  nn.BatchNorm2d(8),\n",
    "                  nn.ReLU()\n",
    "                  ) \n",
    "            #\n",
    "            self.mult_bn1 = nn.BatchNorm1d(161) \n",
    "            self.dense1 = nn.Linear(161,self.q_z_output_dim) #161 - full(13+3)+met, 41-4LJ, 216 - full(13+4)+met+mult\n",
    "            self.dnn_bn1 = nn.BatchNorm1d(self.q_z_output_dim)\n",
    "            self.q_z_mean = nn.Linear(self.q_z_output_dim, self.latent_dim)\n",
    "            self.q_z_logvar = nn.Linear(self.q_z_output_dim, self.latent_dim)\n",
    "            #\n",
    "            self.dense3 = nn.Linear(self.latent_dim, self.q_z_output_dim)\n",
    "            self.dnn_bn3 = nn.BatchNorm1d(self.q_z_output_dim)\n",
    "            self.dense4 = nn.Linear(self.q_z_output_dim, 161)\n",
    "            self.dnn_bn4 = nn.BatchNorm1d(161)\n",
    "            #\n",
    "            self.p_x_nn = nn.Sequential(\n",
    "                  nn.ConvTranspose2d(64, 64, kernel_size=(7,1), stride=(1), padding=(0)),\n",
    "                  nn.BatchNorm2d(64),\n",
    "                  nn.ReLU(),\n",
    "                  nn.ConvTranspose2d(64, 32, kernel_size=(5,1), stride=(1), padding=(0)),\n",
    "                  nn.BatchNorm2d(32),\n",
    "                  nn.ReLU(),\n",
    "                  nn.ConvTranspose2d(32, 16, kernel_size=(3,4), stride=(1), padding=(0))\n",
    "                  ) \n",
    "            # log-det-jacobian = 0 without flows\n",
    "            self.ldj = 0\n",
    "            \n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(5, 50),\n",
    "                nn.LeakyReLU(True),\n",
    "                nn.Linear(50, 30),\n",
    "                nn.LeakyReLU(True),\n",
    "                nn.Linear(30, 20),\n",
    "                nn.LeakyReLU(True),\n",
    "                nn.Linear(20, self.q_z_output_dim)\n",
    "                )\n",
    "            \n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(self.q_z_output_dim, 20),\n",
    "                nn.LeakyReLU(True),\n",
    "                nn.Linear(20, 30),\n",
    "                nn.LeakyReLU(True),\n",
    "                nn.Linear(30, 50),\n",
    "                nn.LeakyReLU(True),\n",
    "                nn.Linear(50, 4)\n",
    "                )\n",
    "    \n",
    "        def encode(self, x, y):\n",
    "          \n",
    "            out = torch.cat((x, y),axis=1)\n",
    "            out = self.encoder(out)\n",
    "            # dense Layer 2\n",
    "            mean  = self.q_z_mean(out)\n",
    "            logvar = self.q_z_logvar(out)\n",
    "            return mean, logvar\n",
    "    \n",
    "        def decode(self, z, y):\n",
    "\n",
    "            out = torch.cat((z, y),axis=1)\n",
    "            out = self.mult_bn1(out)\n",
    "            # dense Layer 3\n",
    "            out = self.dense3(out)\n",
    "            out = self.dnn_bn3(out)\n",
    "            out = torch.relu(out)\n",
    "            # dense Layer 4\n",
    "            out = self.dense4(out)\n",
    "            out = self.dnn_bn4(out)\n",
    "            out = torch.relu(out)\n",
    "            # DeConv\n",
    "            out = self.p_x_nn(out)\n",
    "            out = nn.Sigmoid()\n",
    "            \n",
    "            return out\n",
    "    \n",
    "        def reparameterize(self, mean, logvar):\n",
    "            z = mean + torch.randn_like(mean) * torch.exp(0.5 * logvar)\n",
    "            return z\n",
    "    \n",
    "        def forward(self, x, y):\n",
    "            mean, logvar = self.encode(x, y)\n",
    "            z = self.reparameterize(mean, logvar)\n",
    "            out = self.decode(z)\n",
    "            return out, mean, logvar, self.ldj, z, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e01046ec-01f6-41f1-ab1e-aeb87a5f719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Flow(nn.Module):    \n",
    "    def __init__(self, dim, cnn_layers, kernel_size, test_mode=0, use_revert=True):\n",
    "        super(CNN_Flow, self).__init__()\n",
    "\n",
    "        # prepare reversion matrix\n",
    "        self.usecuda = True\n",
    "        self.use_revert = use_revert\n",
    "        self.R = Variable(torch.from_numpy(np.flip(np.eye(dim), axis=1).copy()).float(), requires_grad=False)\n",
    "        if self.usecuda:\n",
    "            self.R = self.R.cuda()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(cnn_layers):\n",
    "\n",
    "            block = Dilation_Block(dim, kernel_size, test_mode)\n",
    "            self.layers.append(block)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logdetSum = 0\n",
    "        output = x\n",
    "        for i in range(len(self.layers)):\n",
    "            output, logdet = self.layers[i](output)\n",
    "            # revert the dimension of the output after each block\n",
    "            if self.use_revert:\n",
    "                z = output.mm(self.R)\n",
    "            logdetSum += logdet\n",
    "\n",
    "        return z, logdetSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bede5ea-a609-416a-b719-66aec6b784ca",
   "metadata": {},
   "outputs": [],
   "source": [
    " class ConvFlowVAE(ConvNet):   \n",
    "    def __init__(self):\n",
    "        super(ConvFlowVAE, self).__init__()\n",
    "\n",
    "        # Initialize log-det-jacobian to zero\n",
    "        self.log_det_j = 0\n",
    "        self.num_flows = 4#args.num_flows # 6 for chan1\n",
    "        self.kernel_size = 10\n",
    "\n",
    "        flow_k = flows.CNN_Flow\n",
    "\n",
    "        # Normalizing flow layers\n",
    "        self.flow = flow_k(dim=self.latent_dim, cnn_layers=self.num_flows, kernel_size=self.kernel_size, test_mode=self.test_mode)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.met = y\n",
    "\n",
    "        # mean and variance of z\n",
    "        z_mu, z_var = self.encode(x, y)\n",
    "        # sample z\n",
    "        z_0 = self.reparameterize(z_mu, z_var)\n",
    "\n",
    "        # Normalizing flows\n",
    "        z_k, logdet = self.flow(z_0)\n",
    "\n",
    "        x_decoded = self.decode(z_k)\n",
    "\n",
    "        return x_decoded, z_mu, z_var, self.log_det_j, z_0, z_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5956153a-22e2-4b1f-a4fa-ddfd454a4672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTTTTTTT:  (378759, 6)\n",
      "shape of data_train:  (499889, 4)\n",
      "shape of data_test:  (378759, 4)\n",
      "shape of data:  (878648, 4)\n",
      "shape of cond_train (499889,)\n",
      "shape of cond_test (378759,)\n",
      "shape of data:  (878648,)\n",
      "(878648, 4)\n",
      "X (500000, 4) (378648, 4)\n",
      "Y (500000, 1) (378648, 1)\n"
     ]
    }
   ],
   "source": [
    "outerdata_train = np.load(\"/global/u2/a/agarabag/anomoly_studies/CATHODE/separated_data/outerdata_train.npy\")\n",
    "outerdata_test = np.load(\"/global/u2/a/agarabag/anomoly_studies/CATHODE/separated_data/outerdata_test.npy\")\n",
    "\n",
    "outerdata_train = outerdata_train[outerdata_train[:,5]==0]\n",
    "outerdata_test = outerdata_test[outerdata_test[:,5]==0]\n",
    "print (\"TTTTTTTT: \" ,  outerdata_test.shape)\n",
    "nFeat = 4\n",
    "\n",
    "data_train = outerdata_train[:,1:nFeat+1]\n",
    "print('shape of data_train: ', data_train.shape)\n",
    "data_test = outerdata_test[:,1:nFeat+1]\n",
    "print('shape of data_test: ', data_test.shape)\n",
    "\n",
    "data = np.concatenate((data_train, data_test), axis=0)\n",
    "print('shape of data: ', data.shape)\n",
    "\n",
    "cond_data_train = outerdata_train[:,0]\n",
    "print('shape of cond_train', cond_data_train.shape)\n",
    "cond_data_test = outerdata_test[:,0]\n",
    "print('shape of cond_test', cond_data_test.shape)\n",
    "\n",
    "cond_data = np.concatenate((cond_data_train, cond_data_test), axis=0)\n",
    "print('shape of data: ', cond_data.shape)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "max = np.empty(nFeat)\n",
    "for i in range(0,data.shape[1]):\n",
    "\tmax[i] = np.max(np.abs(data[:,i]))\n",
    "\tif np.abs(max[i]) > 0: \n",
    "\t\tdata[:,i] = data[:,i]/max[i]\n",
    "\telse:\n",
    "\t\tpass\n",
    "\n",
    "cond_max = np.max(np.abs(cond_data))\n",
    "if np.abs(cond_max) > 0:\n",
    "    cond_data = cond_data/cond_max\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "trainsize = 500000\n",
    "\t\n",
    "print(np.shape(data))\n",
    "x_train = data[:trainsize]\n",
    "x_test = data[trainsize:]\n",
    "y_train = cond_data[:trainsize]\n",
    "y_test = cond_data[trainsize:]\n",
    "\n",
    "# x_train = np.hstack([x_train,y_train.reshape(y_train.shape[0],1)])\n",
    "# x_test = np.hstack([x_test,y_test.reshape(y_test.shape[0],1)])\n",
    "\n",
    "image_size = x_train.shape[1]\n",
    "original_dim = image_size\n",
    "# print(\"RRRRRRRR: \" , x_trainx.shape)\n",
    "x_train = np.reshape(x_train, [-1, original_dim])\n",
    "# print(\"RRRRRRRR: \" , x_train.shape)\n",
    "x_test = np.reshape(x_test, [-1, original_dim])\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "print('X', x_train.shape, x_test.shape)\n",
    "y_train = np.reshape(y_train, [-1, 1])\n",
    "y_test = np.reshape(y_test, [-1, 1])\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "print('Y', y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec556b8e-79e8-4aa8-b453-7026662a4883",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "x_training = torch.utils.data.DataLoader(dataset = x_train, batch_size = batch_size, shuffle=True)\n",
    "x_testing = torch.utils.data.DataLoader(dataset = x_test, batch_size = batch_size, shuffle=False)\n",
    "\n",
    "y_training = torch.utils.data.DataLoader(dataset = y_train, batch_size = batch_size, shuffle=True)\n",
    "y_testing = torch.utils.data.DataLoader(dataset = y_test, batch_size = batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7086885-9f5c-4892-a49f-c1e0647f5ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "cvae = ConvFlowVAE()\n",
    "if torch.cuda.is_available():\n",
    "    cvae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "283c93ca-fa7c-44d9-b77b-98dd9ec2c1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvFlowVAE(\n",
       "  (q_z_nn): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 4), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 16, kernel_size=(5, 1), stride=(1, 1))\n",
       "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(16, 8, kernel_size=(7, 1), stride=(1, 1))\n",
       "    (7): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "  )\n",
       "  (mult_bn1): BatchNorm1d(161, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dense1): Linear(in_features=161, out_features=128, bias=True)\n",
       "  (dnn_bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (q_z_mean): Linear(in_features=128, out_features=20, bias=True)\n",
       "  (q_z_logvar): Linear(in_features=128, out_features=20, bias=True)\n",
       "  (dense3): Linear(in_features=20, out_features=128, bias=True)\n",
       "  (dnn_bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dense4): Linear(in_features=128, out_features=161, bias=True)\n",
       "  (dnn_bn4): BatchNorm1d(161, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (p_x_nn): Sequential(\n",
       "    (0): ConvTranspose2d(64, 64, kernel_size=(7, 1), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): ConvTranspose2d(64, 32, kernel_size=(5, 1), stride=(1, 1))\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(32, 16, kernel_size=(3, 4), stride=(1, 1))\n",
       "  )\n",
       "  (flow): CNN_Flow(\n",
       "    (layers): ModuleList(\n",
       "      (0): Dilation_Block(\n",
       "        (block): ModuleList(\n",
       "          (0): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,))\n",
       "          )\n",
       "          (1): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,), dilation=(2,))\n",
       "          )\n",
       "          (2): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,), dilation=(4,))\n",
       "          )\n",
       "          (3): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,), dilation=(8,))\n",
       "          )\n",
       "          (4): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,), dilation=(16,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Dilation_Block(\n",
       "        (block): ModuleList(\n",
       "          (0): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,))\n",
       "          )\n",
       "          (1): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,), dilation=(2,))\n",
       "          )\n",
       "          (2): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,), dilation=(4,))\n",
       "          )\n",
       "          (3): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,), dilation=(8,))\n",
       "          )\n",
       "          (4): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,), dilation=(16,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Dilation_Block(\n",
       "        (block): ModuleList(\n",
       "          (0): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,))\n",
       "          )\n",
       "          (1): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,), dilation=(2,))\n",
       "          )\n",
       "          (2): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,), dilation=(4,))\n",
       "          )\n",
       "          (3): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,), dilation=(8,))\n",
       "          )\n",
       "          (4): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,), dilation=(16,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Dilation_Block(\n",
       "        (block): ModuleList(\n",
       "          (0): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,))\n",
       "          )\n",
       "          (1): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,), dilation=(2,))\n",
       "          )\n",
       "          (2): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,), dilation=(4,))\n",
       "          )\n",
       "          (3): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,), dilation=(8,))\n",
       "          )\n",
       "          (4): CNN_Flow_Layer(\n",
       "            (conv1d): Conv1d(1, 1, kernel_size=(10,), stride=(1,), dilation=(16,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b89a24a-0f44-47cb-8d57-9374a2df1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = x_training.cuda().float()\n",
    "met_train = y_training.cuda().float()\n",
    "\n",
    "for epoch in range(self.num_epochs):\n",
    "    self.x_graph.append(epoch)\n",
    "    print('Starting to train ...')\n",
    "\n",
    "    # training\n",
    "    tr_loss_aux = 0.0\n",
    "    tr_kl_aux = 0.0\n",
    "    tr_rec_aux = 0.0\n",
    "    for y, (x_train, met_tr) in tqdm(enumerate(zip(self.train_loader, self.metTr_loader))):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TOARCH",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
